#!/usr/bin/env python3
"""
åŠ¨æ€ WARP æ‰©å®¹ç³»ç»Ÿ
æ ¹æ®æµé‡è´Ÿè½½å’Œå“åº”æ—¶é—´è‡ªåŠ¨è°ƒæ•´ WARP é…ç½®æ•°é‡

âš ï¸ é‡è¦è¯´æ˜ï¼š
1. æ¯ä¸ªè®¾å¤‡æ³¨å†Œéƒ½å‘ Cloudflare å‘é€è¯·æ±‚ï¼Œè¿‡äºé¢‘ç¹å¯èƒ½è§¦å‘é€Ÿç‡é™åˆ¶
2. IP çº§åˆ«çš„é€Ÿç‡é™åˆ¶ï¼šåŒä¸€ IP åœ°å€æ³¨å†Œè¿‡å¤šè®¾å¤‡å¯èƒ½è¢«é™åˆ¶
3. éœ€è¦è°¨æ…æ§åˆ¶æ‰©å®¹é€Ÿåº¦ï¼Œé¿å…è¢« Cloudflare è¯†åˆ«ä¸ºæ»¥ç”¨
4. æ¨èçš„æ‰©å®¹ç­–ç•¥ï¼šæ¸è¿›å¼ã€ä½é¢‘ç‡ã€æœ‰ç›‘æ§
"""

import asyncio
import logging
import time
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from collections import deque

from .warp_api_client import CloudflareWARPAPI
from .warp_optimizer import get_warp_optimizer
from .concurrency_limiter import get_concurrency_limiter

logger = logging.getLogger(__name__)

@dataclass
class ScalingMetrics:
    """æ‰©å®¹æŒ‡æ ‡"""
    avg_response_time: float = 0.0
    active_connections: int = 0
    queue_length: int = 0
    error_rate: float = 0.0
    config_count: int = 0
    timestamp: datetime = None

@dataclass 
class ScalingLimits:
    """æ‰©å®¹é™åˆ¶é…ç½®"""
    # åŸºç¡€é™åˆ¶
    min_configs: int = 5
    max_configs: int = 15  # ä¿å®ˆä¸Šé™ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨
    target_configs: int = 8
    
    # æ€§èƒ½é˜ˆå€¼
    max_response_time: float = 3.0  # ç§’
    max_queue_length: int = 20
    max_error_rate: float = 0.1  # 10%
    
    # æ‰©å®¹æ§åˆ¶
    scale_up_threshold: float = 2.0  # å“åº”æ—¶é—´é˜ˆå€¼
    scale_down_threshold: float = 1.0  # å“åº”æ—¶é—´é˜ˆå€¼
    scale_cooldown: int = 300  # 5åˆ†é’Ÿå†·å´æœŸ
    max_scale_per_hour: int = 3  # æ¯å°æ—¶æœ€å¤šæ‰©å®¹3æ¬¡
    
    # API è°ƒç”¨é™åˆ¶ï¼ˆé˜²æ­¢è§¦å‘ Cloudflare é™åˆ¶ï¼‰
    api_call_interval: int = 60  # æ¯æ¬¡ API è°ƒç”¨é—´éš”
    max_concurrent_registrations: int = 2  # æœ€å¤§å¹¶å‘æ³¨å†Œæ•°

class DynamicWARPScaler:
    """åŠ¨æ€ WARP æ‰©å®¹å™¨"""
    
    def __init__(self, limits: ScalingLimits = None):
        self.limits = limits or ScalingLimits()
        self.metrics_history: deque = deque(maxlen=100)  # ä¿ç•™æœ€è¿‘100ä¸ªæŒ‡æ ‡
        self.last_scale_time: Optional[datetime] = None
        self.scale_operations_per_hour: List[datetime] = []
        self.is_scaling: bool = False
        
        # ä¾èµ–ç»„ä»¶
        self.warp_optimizer = None
        self.concurrency_limiter = None
        
        logger.info("ğŸš€ åŠ¨æ€ WARP æ‰©å®¹å™¨åˆå§‹åŒ–")
    
    async def initialize(self):
        """åˆå§‹åŒ–æ‰©å®¹å™¨"""
        try:
            self.warp_optimizer = get_warp_optimizer()
            self.concurrency_limiter = get_concurrency_limiter()
            
            logger.info("âœ… åŠ¨æ€æ‰©å®¹å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ¨æ€æ‰©å®¹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def collect_metrics(self) -> ScalingMetrics:
        """æ”¶é›†å½“å‰ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # è·å–å¹¶å‘æ§åˆ¶å™¨çŠ¶æ€
            concurrency_status = self.concurrency_limiter.get_status()
            
            # è·å– WARP ä¼˜åŒ–å™¨çŠ¶æ€
            warp_status = self.warp_optimizer.get_optimization_status()
            
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´ï¼ˆæ¨¡æ‹Ÿï¼Œå®é™…åº”è¯¥ä»çœŸå®è¯·æ±‚ä¸­è·å–ï¼‰
            avg_response_time = self._calculate_avg_response_time()
            
            # è®¡ç®—é”™è¯¯ç‡ï¼ˆæ¨¡æ‹Ÿï¼‰
            error_rate = self._calculate_error_rate()
            
            metrics = ScalingMetrics(
                avg_response_time=avg_response_time,
                active_connections=concurrency_status.get("active_requests", 0),
                queue_length=concurrency_status.get("queued_requests", 0),
                error_rate=error_rate,
                config_count=warp_status.get("healthy_configs", 0),
                timestamp=datetime.now()
            )
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.metrics_history.append(metrics)
            
            return metrics
            
        except Exception as e:
            logger.error(f"æ”¶é›†æŒ‡æ ‡å¤±è´¥: {e}")
            return ScalingMetrics(timestamp=datetime.now())
    
    def _calculate_avg_response_time(self) -> float:
        """è®¡ç®—å¹³å‡å“åº”æ—¶é—´ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        if len(self.metrics_history) == 0:
            return 1.0
        
        # åŸºäºé˜Ÿåˆ—é•¿åº¦ä¼°ç®—å“åº”æ—¶é—´
        recent_metrics = list(self.metrics_history)[-10:]  # æœ€è¿‘10ä¸ªæŒ‡æ ‡
        queue_lengths = [m.queue_length for m in recent_metrics]
        
        if not queue_lengths:
            return 1.0
        
        avg_queue = statistics.mean(queue_lengths)
        # ç®€åŒ–è®¡ç®—ï¼šé˜Ÿåˆ—è¶Šé•¿ï¼Œå“åº”æ—¶é—´è¶Šé•¿
        estimated_time = 1.0 + (avg_queue * 0.1)
        
        return min(estimated_time, 10.0)  # æœ€å¤§10ç§’
    
    def _calculate_error_rate(self) -> float:
        """è®¡ç®—é”™è¯¯ç‡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        if len(self.metrics_history) == 0:
            return 0.0
        
        # åŸºäºé…ç½®æ•°é‡å’Œè´Ÿè½½ä¼°ç®—é”™è¯¯ç‡
        recent_metrics = list(self.metrics_history)[-5:]
        total_requests = sum(m.active_connections + m.queue_length for m in recent_metrics)
        config_count = recent_metrics[-1].config_count if recent_metrics else 5
        
        if total_requests == 0:
            return 0.0
        
        # ç®€åŒ–è®¡ç®—ï¼šé…ç½®ä¸è¶³æ—¶é”™è¯¯ç‡å‡é«˜
        if config_count < self.limits.min_configs:
            return 0.2
        elif total_requests > (config_count * 4):  # æ¯ä¸ªé…ç½®æ‰¿è½½è¶…è¿‡4ä¸ªè¯·æ±‚
            return 0.1
        else:
            return 0.02
    
    def should_scale_up(self, metrics: ScalingMetrics) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰©å®¹"""
        if self.is_scaling:
            logger.debug("æ­£åœ¨æ‰©å®¹ä¸­ï¼Œè·³è¿‡")
            return False
        
        # æ£€æŸ¥å†·å´æœŸ
        if self.last_scale_time:
            time_since_last = (datetime.now() - self.last_scale_time).total_seconds()
            if time_since_last < self.limits.scale_cooldown:
                logger.debug(f"å†·å´æœŸå†…ï¼Œå‰©ä½™ {self.limits.scale_cooldown - time_since_last:.0f} ç§’")
                return False
        
        # æ£€æŸ¥æ¯å°æ—¶æ‰©å®¹æ¬¡æ•°é™åˆ¶
        if self._get_scales_in_last_hour() >= self.limits.max_scale_per_hour:
            logger.warning("å·²è¾¾åˆ°æ¯å°æ—¶æœ€å¤§æ‰©å®¹æ¬¡æ•°é™åˆ¶")
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§é…ç½®æ•°
        if metrics.config_count >= self.limits.max_configs:
            logger.debug(f"å·²è¾¾åˆ°æœ€å¤§é…ç½®æ•°é™åˆ¶: {self.limits.max_configs}")
            return False
        
        # æ€§èƒ½æŒ‡æ ‡æ£€æŸ¥
        should_scale = (
            metrics.avg_response_time > self.limits.scale_up_threshold or
            metrics.queue_length > self.limits.max_queue_length or
            metrics.error_rate > self.limits.max_error_rate
        )
        
        if should_scale:
            logger.info(f"æ»¡è¶³æ‰©å®¹æ¡ä»¶: å“åº”æ—¶é—´={metrics.avg_response_time:.2f}s, "
                       f"é˜Ÿåˆ—é•¿åº¦={metrics.queue_length}, é”™è¯¯ç‡={metrics.error_rate:.2%}")
        
        return should_scale
    
    def should_scale_down(self, metrics: ScalingMetrics) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç¼©å®¹"""
        if self.is_scaling:
            return False
        
        # æ£€æŸ¥å†·å´æœŸ
        if self.last_scale_time:
            time_since_last = (datetime.now() - self.last_scale_time).total_seconds()
            if time_since_last < self.limits.scale_cooldown:
                return False
        
        # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å°é…ç½®æ•°
        if metrics.config_count <= self.limits.min_configs:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æŒç»­çš„ä½è´Ÿè½½
        if len(self.metrics_history) < 5:
            return False
        
        recent_metrics = list(self.metrics_history)[-5:]
        all_low_load = all(
            m.avg_response_time < self.limits.scale_down_threshold and
            m.queue_length == 0 and
            m.error_rate < 0.05
            for m in recent_metrics
        )
        
        if all_low_load:
            logger.info("æ£€æµ‹åˆ°æŒç»­ä½è´Ÿè½½ï¼Œè€ƒè™‘ç¼©å®¹")
        
        return all_low_load
    
    def _get_scales_in_last_hour(self) -> int:
        """è·å–è¿‡å»ä¸€å°æ—¶çš„æ‰©å®¹æ¬¡æ•°"""
        one_hour_ago = datetime.now() - timedelta(hours=1)
        self.scale_operations_per_hour = [
            op_time for op_time in self.scale_operations_per_hour 
            if op_time > one_hour_ago
        ]
        return len(self.scale_operations_per_hour)
    
    async def scale_up(self, target_count: int = None) -> bool:
        """æ‰©å®¹æ“ä½œ"""
        if self.is_scaling:
            logger.warning("æ‰©å®¹æ“ä½œå·²åœ¨è¿›è¡Œä¸­")
            return False
        
        self.is_scaling = True
        try:
            current_metrics = self.collect_metrics()
            current_count = current_metrics.config_count
            
            # ç¡®å®šç›®æ ‡æ•°é‡
            if target_count is None:
                target_count = min(current_count + 2, self.limits.max_configs)
            
            add_count = target_count - current_count
            if add_count <= 0:
                logger.info("æ— éœ€æ‰©å®¹")
                return True
            
            logger.info(f"ğŸš€ å¼€å§‹æ‰©å®¹: {current_count} â†’ {target_count} (+{add_count})")
            
            # é™åˆ¶å¹¶å‘APIè°ƒç”¨ï¼Œé¿å…è§¦å‘Cloudflareé™åˆ¶
            semaphore = asyncio.Semaphore(self.limits.max_concurrent_registrations)
            
            async def add_single_config(index: int) -> bool:
                async with semaphore:
                    try:
                        logger.info(f"æ­£åœ¨ç”Ÿæˆç¬¬ {index+1} ä¸ªæ–°é…ç½®...")
                        
                        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹çš„APIè°ƒç”¨
                        if index > 0:
                            await asyncio.sleep(self.limits.api_call_interval)
                        
                        config_path = await self.warp_optimizer.warp_manager.add_new_config(
                            f"warp_scale_{int(time.time())}_{index+1:02d}.conf"
                        )
                        
                        if config_path:
                            logger.info(f"âœ… æˆåŠŸæ·»åŠ é…ç½®: {config_path}")
                            return True
                        else:
                            logger.error(f"âŒ æ·»åŠ é…ç½®å¤±è´¥: {index+1}")
                            return False
                            
                    except Exception as e:
                        logger.error(f"æ·»åŠ é…ç½®å¼‚å¸¸ {index+1}: {e}")
                        return False
            
            # å¹¶å‘æ·»åŠ é…ç½®
            tasks = [add_single_config(i) for i in range(add_count)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            success_count = sum(1 for r in results if r is True)
            
            # æ›´æ–°è®°å½•
            self.last_scale_time = datetime.now()
            self.scale_operations_per_hour.append(self.last_scale_time)
            
            # è§¦å‘ä¼˜åŒ–å™¨æ›´æ–°
            await self.warp_optimizer.force_optimization()
            
            logger.info(f"âœ… æ‰©å®¹å®Œæˆ: æˆåŠŸæ·»åŠ  {success_count}/{add_count} ä¸ªé…ç½®")
            return success_count > 0
            
        except Exception as e:
            logger.error(f"âŒ æ‰©å®¹æ“ä½œå¤±è´¥: {e}")
            return False
        finally:
            self.is_scaling = False
    
    async def scale_down(self, target_count: int = None) -> bool:
        """ç¼©å®¹æ“ä½œ"""
        if self.is_scaling:
            logger.warning("ç¼©å®¹æ“ä½œå·²åœ¨è¿›è¡Œä¸­")
            return False
        
        self.is_scaling = True
        try:
            current_metrics = self.collect_metrics()
            current_count = current_metrics.config_count
            
            # ç¡®å®šç›®æ ‡æ•°é‡
            if target_count is None:
                target_count = max(current_count - 1, self.limits.min_configs)
            
            remove_count = current_count - target_count
            if remove_count <= 0:
                logger.info("æ— éœ€ç¼©å®¹")
                return True
            
            logger.info(f"ğŸ“‰ å¼€å§‹ç¼©å®¹: {current_count} â†’ {target_count} (-{remove_count})")
            
            # ç¼©å®¹é€»è¾‘ï¼ˆç§»é™¤ä¸å¥åº·çš„é…ç½®ï¼‰
            result = await self.warp_optimizer.force_optimization()
            
            self.last_scale_time = datetime.now()
            self.scale_operations_per_hour.append(self.last_scale_time)
            
            logger.info(f"âœ… ç¼©å®¹å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç¼©å®¹æ“ä½œå¤±è´¥: {e}")
            return False
        finally:
            self.is_scaling = False
    
    async def monitor_and_scale(self):
        """ç›‘æ§å¹¶æ‰§è¡Œè‡ªåŠ¨æ‰©ç¼©å®¹"""
        try:
            # æ”¶é›†æŒ‡æ ‡
            metrics = self.collect_metrics()
            
            logger.debug(f"æŒ‡æ ‡: å“åº”æ—¶é—´={metrics.avg_response_time:.2f}s, "
                        f"æ´»è·ƒè¿æ¥={metrics.active_connections}, "
                        f"é˜Ÿåˆ—é•¿åº¦={metrics.queue_length}, "
                        f"é”™è¯¯ç‡={metrics.error_rate:.2%}, "
                        f"é…ç½®æ•°={metrics.config_count}")
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰©å®¹
            if self.should_scale_up(metrics):
                await self.scale_up()
            elif self.should_scale_down(metrics):
                await self.scale_down()
            
        except Exception as e:
            logger.error(f"ç›‘æ§æ‰©å®¹å¤±è´¥: {e}")
    
    async def start_monitoring(self, interval: int = 60):
        """å¼€å§‹ç›‘æ§å¾ªç¯"""
        logger.info(f"ğŸ”„ å¼€å§‹è‡ªåŠ¨æ‰©å®¹ç›‘æ§ (é—´éš”: {interval}ç§’)")
        
        while True:
            try:
                await self.monitor_and_scale()
                await asyncio.sleep(interval)
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                await asyncio.sleep(interval)
    
    def get_status(self) -> Dict:
        """è·å–æ‰©å®¹å™¨çŠ¶æ€"""
        current_metrics = self.collect_metrics() if self.metrics_history else ScalingMetrics()
        
        return {
            "scaling_limits": {
                "min_configs": self.limits.min_configs,
                "max_configs": self.limits.max_configs,
                "target_configs": self.limits.target_configs,
                "max_response_time": self.limits.max_response_time,
                "scale_cooldown": self.limits.scale_cooldown,
                "max_scale_per_hour": self.limits.max_scale_per_hour
            },
            "current_metrics": {
                "avg_response_time": current_metrics.avg_response_time,
                "active_connections": current_metrics.active_connections,
                "queue_length": current_metrics.queue_length,
                "error_rate": current_metrics.error_rate,
                "config_count": current_metrics.config_count
            },
            "scaling_status": {
                "is_scaling": self.is_scaling,
                "last_scale_time": self.last_scale_time.isoformat() if self.last_scale_time else None,
                "scales_in_last_hour": self._get_scales_in_last_hour(),
                "metrics_history_count": len(self.metrics_history)
            },
            "recommendations": self._get_recommendations(current_metrics)
        }
    
    def _get_recommendations(self, metrics: ScalingMetrics) -> List[str]:
        """è·å–æ‰©å®¹å»ºè®®"""
        recommendations = []
        
        if metrics.avg_response_time > self.limits.scale_up_threshold:
            recommendations.append(f"å“åº”æ—¶é—´è¿‡é«˜ ({metrics.avg_response_time:.2f}s)ï¼Œå»ºè®®æ‰©å®¹")
        
        if metrics.queue_length > self.limits.max_queue_length:
            recommendations.append(f"é˜Ÿåˆ—è¿‡é•¿ ({metrics.queue_length})ï¼Œå»ºè®®æ‰©å®¹")
        
        if metrics.error_rate > self.limits.max_error_rate:
            recommendations.append(f"é”™è¯¯ç‡è¿‡é«˜ ({metrics.error_rate:.2%})ï¼Œå»ºè®®æ‰©å®¹")
        
        if metrics.config_count < self.limits.min_configs:
            recommendations.append(f"é…ç½®æ•°ä¸è¶³ ({metrics.config_count})ï¼Œå»ºè®®è¡¥å……åˆ°æœ€å°‘ {self.limits.min_configs} ä¸ª")
        
        if metrics.config_count > self.limits.max_configs:
            recommendations.append(f"é…ç½®æ•°è¿‡å¤š ({metrics.config_count})ï¼Œå»ºè®®å‡å°‘åˆ° {self.limits.max_configs} ä¸ªä»¥ä¸‹")
        
        if self._get_scales_in_last_hour() >= self.limits.max_scale_per_hour:
            recommendations.append("å·²è¾¾åˆ°æ¯å°æ—¶æ‰©å®¹æ¬¡æ•°é™åˆ¶ï¼Œè¯·ç­‰å¾…")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½")
        
        return recommendations


# å…¨å±€å®ä¾‹
_scaler: Optional[DynamicWARPScaler] = None

def get_dynamic_scaler(limits: ScalingLimits = None) -> DynamicWARPScaler:
    """è·å–å…¨å±€åŠ¨æ€æ‰©å®¹å™¨å®ä¾‹"""
    global _scaler
    if _scaler is None:
        _scaler = DynamicWARPScaler(limits)
    return _scaler

async def start_auto_scaling(interval: int = 60):
    """å¯åŠ¨è‡ªåŠ¨æ‰©å®¹"""
    scaler = get_dynamic_scaler()
    await scaler.initialize()
    await scaler.start_monitoring(interval) 